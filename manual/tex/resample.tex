\chapter{Resampling}
\label{chap:Resampling}

\section{Builtin algorithms}
\label{sec:Builtin algorithms}

The library supports resampling in a more general way than the algorithm
described in Chapter~\ref{chap:Sequential Monte Carlo}. Recall that, given a
particle system $\{W^{(i)},X^{(i)}\}_{i=1}^N$, a new system $\{\bar{W}^{(i)},
\bar{X}^{(i)}\}_{i=1}^M$ is generated. Regardless of other statistical
properties, in practice, such an algorithm can be decomposed into three steps.
First, a vector of replication numbers $\{r_i\}_{i=1}^N$ is generated such that
$\sum_{i=1}^N r_i = M$, and $0 \le r_i \le M$ for $i=1,\dots,N$. Then a vector
of indices $\{a_i\}_{i=1}^M$ is generated such that $\sum_{i=1}^M
\bbI_{\{j\}}(a_i) = r_j$, and $1 \le a_i \le N$ for $i= 1,\dots,M$. And last,
set $\bar{X}^{(i)} = X^{(a_i)}$.

The first step determines the statistical properties of the resampling
algorithm. The library defines all algorithms discussed in
\textcite{Douc:2005wa}. Samplers can be constructed with builtin schemes as
seen in Section~\ref{sub:Implementations}. In addition, samplers can also be
constructed with user defined resampling operations. A user defined resampling
algorithm can be any type that is convertible to
\verb|Sampler<T>::resample_type|,
following function call,
\begin{Verbatim}
  using resample_type = std::function<void(std::size_t, std::size_t,
      typename Particle<T>::rng_type &, const double *, size_type *)>;
\end{Verbatim}
where the first argument is $N$, the sample size before resampling; the second
is $M$, the sample size after resampling; the third is a \cppoo{} \rng type,
the fourth is a pointer to normalized weight, and the last is a pointer to the
vector $\{r_i\}_{i=1}^N$. The builtin schemes are implemented as classes with
\verb|operator()| conforms to the above signature. All builtin schemes are
listed in Table~\ref{tab:Resampling schemes}

\begin{table}
  \begin{tabularx}{\textwidth}{lL}
    \toprule
    \verb|ResampleScheme| & Algorithm \\
    \midrule
    \verb|Multinomial|        & Multinomial resampling             \\
    \verb|Stratified|         & Stratified resampling              \\
    \verb|Systematic|         & Systematic resampling              \\
    \verb|Residual|           & Residual resampling                \\
    \verb|ResidualStratified| & Stratified resampling on residuals \\
    \verb|ResidualSystematic| & Systematic resampling on residuals \\
    \bottomrule
  \end{tabularx}
  \caption{Resampling schemes}
  \label{tab:Resampling schemes}
\end{table}

To transform $\{r_i\}_{i=1}^N$ into $\{a_i\}_{i=1}^M$, one can call the
following function,
\begin{Verbatim}
  template <typename IntType1, typename IntType2>
  void resample_trans_rep_index(std::size_t N, std::size_t M,
      const IntType1 *replication, IntType2 *index);
\end{Verbatim}
where the last parameter is the output vector $\{a_i\}_{i=1}^M$. This function
guarantees that $a_i = i$ if $r_i > 0$, for $i = 0,\dots,\min\{N, M\}$.
However, its output may not be optimal for all applications. The last step of a
resampling operation, the copying of particles can be the most time consuming
one, especially on distributed systems. The topology of the system will need to
be taking into consideration to achieve optimal performance. In those
situations, it is best to use \verb|ResampleMultinomial| etc., to generate the
replication numbers, and manually perform the rest of the resampling algorithm.

\section{User defined algorithms}
\label{sec:User defined algorithms}

The library provides facilities for implementing new resampling algorithms.
The most common situation is that, a vector of random numbers on the interval
$[0, 1)$ is generated, say $\{u_i\}_{i=1}^M$. The replication numbers are $r_i
= \sum_{j=1}^M \bbI_{[v_{i-1},v_i)}(u_i)$, where $v_i = \sum_{j=1}^i W_i$, $v_0
= 0$. For example, the Multinomial resampling algorithm is equivalent to
$\{u_i\}_{i=1}^M$ being i.i.d.\ standard uniform random numbers.

Alternatively, let $p_i = \Floor{MW_i}$, $q_i = MW_i - p_i$. One can perform
resampling on the residuals, using weights proportional to $\{q_i\}_{i=1}^N$.
The output size shall be $R = M - \sum_{i=1}^N p_i$. Let the replication
numbers be $\{s_i\}_{i=1}^N$, then $r_i = p_i + s_i$.

The library provides the following class template for implementing such
algorithms,
\begin{Verbatim}
  template <typename U01SeqType, bool Residual>
  class ResampleAlgorithm;
\end{Verbatim}
where \verb|U01SeqType| will be discussed later. The second parameter
\verb|Residual| determines if the resampling shall be applied to residuals.

The template template parameter \verb|U01SeqType| shall be a class with default
construct that defines the following an \verb|operator()| that is compatible
with the following,
\begin{Verbatim}
  template <typename RNGType>
  void operator()(RNGType &rng, std::size_t N, double *r);
\end{Verbatim}
which will generate $N$ random numbers within $[0, 1]$ such, say
$\{U_i\}_{i=1}^N$, such that $U_1 \le U_2 \le \dots \le U_N$.

An obvious method is to generate the random numbers first and then sort them.
However, no sorting algorithm has cost $\calO(M)$, while it is possible to
generate such an ordered sequence with cost $\calO(M)$ by using order
statistics. The library defines three such sequences. The first is equivalent
to sorted i.i.d.\ random numbers. The second and the third are stratified and
systematic, respectively. The builtin resampling algorithms are implemented
using these sequences. For example,
\begin{Verbatim}
  using ResampleMultinomial = ResampleAlgorithm<U01SequenceSorted, false>;
  using ResampleResidual = ResampleAlgorithm<U01SequenceSorted, true>;
\end{Verbatim}
If the user is able to define a new ordered random sequence, either through
sorting or otherwise, then using the \verb|ResampleAlgorithm| template, a new
resampling algorithm can easily be implemented. Note that, the algorithm
implemented by this class template always has a cost $\calO(N + M)$, unless the
random sequence has a greater cost.

\section{Algorithms with increasing dimensions}
\label{sec:Algorithms with increasing dimensions}

In general an \sis algorithm operates on increasing dimensions. Assume that the
storage cost of a single particle at the marginal ($X_t^{(i)}$) is of order
$\calO(1)$. At each iterations $t$, the path $X_{0:t-1}^{(i)}$ is extended to
$X_{0:t}^{(i)}$. The resampling algorithms operate on the space
$\prod_{k=0}^tE_k$. When the proposal $q_t(\cdot|X_{0:t-1}^{(i)}) =
q_t(\cdot|X_{t-1}^{(i)})$ and only the marginal $\eta_t(X_t)$ is of interest,
one can only resample $\{X_t^{(i)}\}_{i=1}^N$. This leads to $O(N)$ cost for
resampling. This is the typical case for \smc algorithms. However, there are
situations where resampling $\{X_{0:t}^{(i)}\}_{i=1}^N$ is necessary. In this
case, the cost of resampling at iteration $t$ is $O(tN)$. And total resampling
cost to obtain $\{X_{0:t}^{(i)}\}_{i=1}^N$, is $O(t^2N)$.

However, such cost is avoidable in some circumstances. Recall that, after
generating the resampling index $\{a_i\}_{i=1}^N$, one set $\bar{X}_{0:t}^{(i)}
= X_{0:t}^{(a_i)}$. Let $(\{X_0^{(i)}\}_{i=1}^N,\dots,\{X_t^{(i)}\}_{i=1}^N)$
be the marginals before resampling, and
$(\{a_0^{(i)}\}_{i=1}^N,\dots,\{a_t^{(i)}\}_{i=1}^N)$ be the resampling index
vectors at each iteration. Then one can obtain $X_{0:t}^{(i)}$ through
the following recursion,
\begin{align*}
  b_t^{(i)} &= a_t^{(i)} & \\
  b_k^{(i)} &= a_{k}^{(b_{k + 1}^{(i)})} \text{ for } k = t - 1,\dots,0 \\
  \bar{X}_k^{(i)} &= X_k^{(b_k^{(i)})}   \text{ for } k = t,\dots,0\ \\
\end{align*}
Intuitively, only the resampling index vectors are resampled. The cost of the
above recursion is $O(tN)$ instead of $O(t^2N)$. Not that it is likely to be
much slower compared to directly copying particles at each iteration in the
situation when only the marginals need to be resampled, in which case both has
a cost $O(tN)$.

This algorithm is useful in the following situation. Assume that there exist
recursive functions,
\begin{align*}
\varphi_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)}) &=
\varphi_t(X_t^{(i)}, \varphi_{t-1}(X_{0:t-1}^{(i)})), \\
\phi_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)}) &=
\phi_t(X_t^{(i)}, \varphi_{t-1}(X_{0:t-1}^{(i)}), \phi_{t-1}(X_{0:t-1}^{(i)})),
\end{align*}
such that
$q(\cdot|X_{0:t}^{(i)}) = q(\cdot|\varphi(X_{0:t-1}^{(i)}))$ and
$W_t(X_{0:t}^{(i)}) = W_t(\phi(X_{0:t}^{(i)}))$. In this case, only the values
of $\varphi_t(X_{0:t}^{(i)})$ and $\phi_t(X_{0:t}^{(i)})$ need to be resampled
at every iteration. If they have storage costs at the order of $\calO(1)$, then
the total cost of resampling will still be $O(tN)$. See \textcite{stpf} for
some examples of such algorithms.

The library provides the following class template for implementing such an
algorithm.
\begin{Verbatim}
template <typename IntType = std::size_t>
class ResampleIndex;
\end{Verbatim}
Its usage is demonstrated by the following example (assuming
$X_t^{(i)}\in\Real$, and $\phi_t$ is the same as $\varphi_t$),
\begin{Verbatim}
  using StateBase = StateMatrix<ColMajor, Dynamic, double>;

  class State : StateBase
  {
      public:
      StateBase(std::size_t N) : StateBase(N), varphi_(N) {}

      template <typename IntType>
      void copy(std::size_t N, IntType *index)
      {
          // DO NOT CALL StateBase::copy
          for (std::size_t i = 0; i != N; ++i)
              varphi_[i] = varphi_[index[i]];
          index_.push_back(N, index);
      }

      // To be called during initialization
      void reset() { index_.reset(); }

      // Get the resampled state up to time n
      // Assuming that this->state(i, t) contains the marginal $X_t^{(i)}$
      State trace_back(std::size_t n)
      {
          // idxmat is an $N$ by $n + 1$ matrix, say $B$, such that
          // $B_{i,j} = b_j^{(i)}$
          auto idxmat = index_.index_matrix(ColMajor, n);
          State rs(*this);
          for (std::size_t j = 0; j <= n; ++j) {
              auto dst = rs.col_data(j);
              auto src = this->col_data(j);
              auto idx = idxmat.data() + j * this->size();
              for (std::size_t i = 0; i != this->size(); ++i)
                  dst[i] = src[idx[i]];
          }

          return rs;
      }

      private:
      Vector<double> varphi_; // the values of $\varphi(X_t^{(i)})$
      ResampleIndex index_;
  };

  Sampler<State> sampler(N, Multinomial); // Always resampling
  sampler.particle.value().resize_dim(n + 1);
  // configure the sampler
  sampler.initialize(param);
  sampler.iterate(n);
  auto state = sampler.particle().value().trace_back(n);
\end{Verbatim}
The method call \verb|index_.push_back(N, index)| append a new resampling index
vector to the history being recorded by \verb|index_|. If called without the
second argument, i.e., \verb|index_.push_back(N)|, then it is assumed $a_i = i$
for $i = 1,\dots,N$. To retrieve $b_{t_0}^{(i)}$, where $t_0 \le t$, one can
call \verb|index_.index(i, t, t0)|. If the last argument is omitted, it is
assumed to be zero. If the second argument is also omitted, then it is assumed
to be the iteration number of the last index vector recorded. It is of course
more useful, and more efficient to retrieve an $N$ by $R$ matrix $B$, such that
$R = t - t_0 + 1$, $B_{i,j} = b_j^{(i)}$. This is done by calling
\verb|index_.index_matrix(t, t0)|. Again, both arguments can be omitted, and
the default values are the same as for \verb|index_.index(i, t, t0)|.

The performance difference directly copy $X_{0:t}^{(i)}$ at each iteration and
using the above implementation can be significant for moderate to large $t$. Of
course, if $\eta_k(X_{0:t})$ is of interest for all $k \le t$, instead of only
$\eta_t(X_{0:t})$ being of interest, then one is better off to copy all states
at all iterations.

Note that, \verb|ResampleIndex| is capable of dealing with varying sample size
situations. Each call of \verb|push_back| does not need to have the same sample
size $N$. In addition, if such an index object need to be reused multiple
times, one can use its \verb|insert| method instead of \verb|push_back|. See
the reference manual for details.
